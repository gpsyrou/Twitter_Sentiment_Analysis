{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data and Sentiment Analysis on COVID-19 related tweets through the TwitterAPI\n",
    "\n",
    "### Author: George Spyrou\n",
    "### Date: 01/03/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this project is to leverage the TwitterAPI functionality offered by Twitter, and conduct an analysis on tweets that are related with the COVID-19 virus. Initially, this project started as an exploratory task to learn how the TwitterAPI can be used to retrieve data (tweets) from the web, and how to use the tweepy and searchtweets python packages. \n",
    "\n",
    "After I managed to retrieve the data, I found myself really interested into performing some data analysis on the retrieved tweets. COVID-19 - or as it's been commonly known as coronavirus - is one of the most discussed topics in Twitter for the period 01/01/2020 - 01/03/2020. Using relevant tweets we want to perform some exploratory data analysis (e.g. find the most common words used in tweets related to covid-19 or identify the bigrams) and then attempt to identify the sentiment of the tweets by using a variety of methods.\n",
    "\n",
    "#### Version 1: The first version has been completed on 01/03/2020 and it includes analysis on:\n",
    "    - Most common words present in tweets.\n",
    "    - Most common bigrams (i.e. pairs of words that often appear next to each other).\n",
    "    - Sentiment analysis by using the Liu Hu opinion lexicon algorithm.\n",
    "    \n",
    "At the first part of the project, we deal with setting up the environment required for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Plots and graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the project environment\n",
    "\n",
    "# Secure location of the required keys to connect to the API\n",
    "# This config also contains the search query (in this case 'coronavirus')\n",
    "json_loc = '/Users/georgiosspyrou/Desktop/config_tweets/Twitter/twitter_config.json'\n",
    "\n",
    "with open(json_loc) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Project folder location and keys\n",
    "os.chdir(data[\"project_directory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we had to create a variety of functions, some of which have been used in order to retrieve/clean the data, as well as the functions that we have used for our main analysis and plotting. For more information regarding this functions, please refer to the **twitterCustomFunc.py** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom functions that we will use to retrieve and analyse the data\n",
    "\n",
    "import twitterCustomFunc as twf\n",
    "\n",
    "twitter_keys_loc = data[\"keys\"]\n",
    "\n",
    "# Load the credentials to get access to the API\n",
    "premium_search_args = load_credentials(twitter_keys_loc,\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)\n",
    "print(premium_search_args)\n",
    "\n",
    "# Set tweet extraction period and create a list of days of interest\n",
    "fromDate = \"2020-02-21\"\n",
    "toDate = \"2020-02-25\"\n",
    "\n",
    "daysList = [fromDate]\n",
    "\n",
    "while fromDate != toDate:\n",
    "    date = datetime.strptime(fromDate, \"%Y-%m-%d\")\n",
    "    mod_date = date + timedelta(days=1)\n",
    "    incrementedDay = datetime.strftime(mod_date, \"%Y-%m-%d\")\n",
    "    daysList.append(incrementedDay)\n",
    "    \n",
    "    fromDate = incrementedDay\n",
    "\n",
    "# Retrieve the data for each day from the API\n",
    "for day in daysList:\n",
    "    \n",
    "    dayNhourList = twf.createDateTimeFrame(day, hourSep=2)\n",
    "    \n",
    "    for hs in dayNhourList:\n",
    "        fromDate = hs[0]\n",
    "        toDate = hs[1]\n",
    "        # Create the searching rule for the stream\n",
    "        rule = gen_rule_payload(pt_rule=data['search_query'],\n",
    "                                from_date=fromDate,\n",
    "                                to_date=toDate ,\n",
    "                                results_per_call = 100)\n",
    "\n",
    "        # Set up the stream\n",
    "        rs = ResultStream(rule_payload=rule,\n",
    "                            max_results=100,\n",
    "                            **premium_search_args)\n",
    "\n",
    "        # Create a .jsonl with the results of the Stream query\n",
    "        #file_date = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "        file_date = '_'.join(hs).replace(' ', '').replace(':','')\n",
    "        filename = os.path.join(data[\"outputFiles\"],f'twitter_30day_results_{file_date}.jsonl')\n",
    "    \n",
    "        # Write the data received from the API to a file\n",
    "        with open(filename, 'a', encoding='utf-8') as f:\n",
    "            cntr = 0\n",
    "            for tweet in rs.stream():\n",
    "                cntr += 1\n",
    "                if cntr % 100 == 0:\n",
    "                    n_str, cr_date = str(cntr), tweet['created_at']\n",
    "                    print(f'\\n {n_str}: {cr_date}')\n",
    "                    json.dump(tweet, f)\n",
    "                    f.write('\\n')\n",
    "        print(f'Created file {f}:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read the data from the created jsonl files\n",
    "jsonl_files_folder = os.path.join(data[\"project_directory\"], data[\"outputFiles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List that will contain all the Tweets that we managed to receive via the use of the API\n",
    "allTweetsList = []\n",
    "\n",
    "for file in os.listdir(jsonl_files_folder):\n",
    "    if 'twitter' in file:\n",
    "        tweets_full_list = twf.loadJsonlData(os.path.join(jsonl_files_folder, file))\n",
    "        allTweetsList += tweets_full_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
