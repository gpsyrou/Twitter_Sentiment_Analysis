{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data and Sentiment Analysis on COVID-19 related tweets via TwitterAPI\n",
    "\n",
    "### Author: George Spyrou\n",
    "### Date: 01/03/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this project is to leverage the TwitterAPI functionality offered by Twitter, and conduct an analysis on tweets that are related with the COVID-19 virus. Initially, this project started as an exploratory task to learn how the TwitterAPI can be used to retrieve data (tweets) from the web, and how to use the tweepy and searchtweets python packages. \n",
    "\n",
    "After I managed to retrieve the data, I found myself really interested into performing some data analysis on the retrieved tweets. COVID-19 - or as it's been commonly known as coronavirus - is one of the most discussed topics in Twitter for the period 01/01/2020 - 01/03/2020. Using relevant tweets, we want to perform some exploratory data analysis (e.g. find the most common words used in tweets related to covid-19 or identify the bigrams) and then attempt to identify the sentiment of the tweets by using a variety of methods.\n",
    "\n",
    "#### Version 1: The first version has been completed on 01/03/2020 and it includes analysis on:\n",
    "- Most common words present in tweets.\n",
    "- Most common bigrams (i.e. pairs of words that often appear next to each other).\n",
    "- Sentiment analysis by using the Liu Hu opinion lexicon algorithm.\n",
    "    \n",
    "At the first part of the project, we deal with setting up the environment required for our analysis, as well as retrieving the data by using the TwitterAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependenciescle\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime  import datetime, timedelta\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Plots and graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "# NLTK module for text preprocessing and analysis\n",
    "from nltk import word_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "from plotly.offline import plot\n",
    "\n",
    "# Set up the project environment\n",
    "\n",
    "# Secure location of the required keys to connect to the API\n",
    "# This config also contains the search query (in this case 'coronavirus')\n",
    "json_loc = 'C:\\\\Users\\\\george\\\\Desktop\\\\Twitter_Project\\\\Twitter\\\\twitter_config.json'\n",
    "\n",
    "\n",
    "with open(json_loc) as json_file:\n",
    "    configFile = json.load(json_file)\n",
    "\n",
    "# Project folder location and keys\n",
    "os.chdir(configFile[\"project_directory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we had to create a variety of functions, some of which have been used in order to retrieve/clean the data, as well as the functions that we have used for our main analysis and plotting. For more information regarding this functions, please refer to the **twitterCustomFunc.py** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom functions that we will use to retrieve and analyse the data\n",
    "\n",
    "import twitterCustomFunc as twf\n",
    "\n",
    "from searchtweets import load_credentials\n",
    "from searchtweets import gen_rule_payload\n",
    "from searchtweets import ResultStream\n",
    "\n",
    "twitter_keys_loc = configFile[\"keys\"]\n",
    "\n",
    "# Load the credentials to get access to the API\n",
    "premium_search_args = load_credentials(twitter_keys_loc,\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)\n",
    "print(premium_search_args)\n",
    "\n",
    "\n",
    "# Set tweet extraction period and create a list of days of interest\n",
    "fromDate = \"2020-03-16\"\n",
    "toDate = \"2020-03-18\"\n",
    "\n",
    "daysList = [fromDate]\n",
    "\n",
    "while fromDate != toDate:\n",
    "    date = datetime.strptime(fromDate, \"%Y-%m-%d\")\n",
    "    mod_date = date + timedelta(days=2)\n",
    "    incrementedDay = datetime.strftime(mod_date, \"%Y-%m-%d\")\n",
    "    daysList.append(incrementedDay)\n",
    "    \n",
    "    fromDate = incrementedDay\n",
    "\n",
    "# Retrieve the data for each day from the API\n",
    "for day in daysList:\n",
    "    \n",
    "    dayNhourList = twf.createDateTimeFrame(day, hourSep=2)\n",
    "    \n",
    "    for hs in dayNhourList:\n",
    "        fromDate = hs[0]\n",
    "        toDate = hs[1]\n",
    "        # Create the searching rule for the stream\n",
    "        rule = gen_rule_payload(pt_rule=configFile['search_query'],\n",
    "                                from_date=fromDate,\n",
    "                                to_date=toDate ,\n",
    "                                results_per_call = 100)\n",
    "\n",
    "        # Set up the stream\n",
    "        rs = ResultStream(rule_payload=rule,\n",
    "                            max_results=100,\n",
    "                            **premium_search_args)\n",
    "\n",
    "        # Create a .jsonl with the results of the Stream query\n",
    "        #file_date = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "        file_date = '_'.join(hs).replace(' ', '').replace(':','')\n",
    "        filename = os.path.join(configFile[\"outputFiles\"],\n",
    "                                f'twitter_30day_results_{file_date}.jsonl')\n",
    "    \n",
    "        # Write the data received from the API to a file\n",
    "        with open(filename, 'a', encoding='utf-8') as f:\n",
    "            cntr = 0\n",
    "            for tweet in rs.stream():\n",
    "                cntr += 1\n",
    "                if cntr % 100 == 0:\n",
    "                    n_str, cr_date = str(cntr), tweet['created_at']\n",
    "                    print(f'\\n {n_str}: {cr_date}')\n",
    "                    json.dump(tweet, f)\n",
    "                    f.write('\\n')\n",
    "        print(f'Created file {f}:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have leveraged the TwitterAPI in order to retrieve tweets relevant to coronavirus for a specific period of time. As the free version of TwitterAPI does not allow us to retrieve as much data as we want, we had to find a workaround on how to collect the dataset. Therefore, we made multiple calls to the API, each time targeting a different day and time. \n",
    "\n",
    "Specifically, in order to make our data collection less biased to a specific day (for example avoid multiple tweets refering to the same news for the coronavirus) we made four calls for each day in our analysis, each call targeting a specific time of the day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path which contains the created jsonl files\n",
    "jsonl_files_folder = os.path.join(configFile[\"project_directory\"], configFile[\"outputFiles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadJsonlData(file: str) -> list:\n",
    "    '''\n",
    "    Reads the data as saved in a .jsonl file\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "    file: String corresponding to the path to a .jsonl file which contains the \n",
    "          tweets as received from the TwitterAPI.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tweets: A list of all the data saved in the .jsonl file.\n",
    "    '''\n",
    "    \n",
    "    tweets = []\n",
    "    with open(file, 'rb') as f:\n",
    "        for tweet in json_lines.reader(f, broken=True):\n",
    "            try:\n",
    "                tweets.append(tweet)\n",
    "            except json_lines.UnicodeDecodeError or json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List that will contain all the Tweets that we managed to receive via the use of the API\n",
    "import json_lines\n",
    "allTweetsList = []\n",
    "\n",
    "for file in os.listdir(jsonl_files_folder):\n",
    "    if 'twitter' in file:\n",
    "        tweets_full_list = loadJsonlData(os.path.join(jsonl_files_folder, file))\n",
    "        allTweetsList += tweets_full_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have collected all the tweets and created a merged list that contains all the relevant information.Each separate case of tweets contains a variety of information, like the name/id of the person who made the tweets, their location, time, and many more. We can have a look inside the first case, and decide which information seems relevant for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTweetsList[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the example above, there is a wide variety of information that we can focus on. For our analysis, we will proceed further with the users **screen name**, **id**, **location**, **day/time** of the tweet, the **tweet** that they made, and the name of the person that they are replying to (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURL(text: str) -> str:\n",
    "    '''\n",
    "    Removes URLs (strings that start with 'http\\\\ or htpps\\\\) from text.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    text: Input string the we want to remove the URL from.\n",
    "     \n",
    "    Returns:\n",
    "    -------\n",
    "    text: The input string clean from any URL.\n",
    "    '''\n",
    "\n",
    "    regex = r'http[0-9a-zA-Z\\\\/.:]+.'\n",
    "    urllinks = re.findall(regex, text)\n",
    "    if  urllinks != []:\n",
    "        for url in urllinks:\n",
    "            print(f'String removed: {url}')\n",
    "            if type(url) is tuple:\n",
    "                url = [x for x in url if x != '']\n",
    "            try:\n",
    "                text = text.replace(url,'')\n",
    "            except TypeError:\n",
    "                continue\n",
    "        return text\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we will create a dataframe based on the relevant data from the full list of received tweets. There is plenty of information in the data received, but we will focus only in a few attributes (features) to create our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ls, userid_ls, tweet_ls = [], [], []\n",
    "location_ls, datetime_ls, replyto_ls = [], [], []\n",
    "geo_loc_ls = []\n",
    "\n",
    "for tweet_dict in allTweetsList:\n",
    "    user_ls.append(tweet_dict['user']['screen_name'])\n",
    "    userid_ls.append(tweet_dict['user']['id'])\n",
    "    tweet_ls.append(twf.removeURL(tweet_dict['text']))\n",
    "    replyto_ls.append(tweet_dict['in_reply_to_user_id'])\n",
    "    location_ls.append(tweet_dict['user']['location'])\n",
    "    datetime_ls.append(tweet_dict['created_at'])\n",
    "    geo_loc_ls.append(tweet_dict['geo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe that contains the data for analysis\n",
    "# Note: The twitter API functionality is very broad in what data we can analyse\n",
    "# This project will focus on tweets and with their respective location/date.\n",
    "df = pd.DataFrame(list(zip(user_ls, userid_ls, tweet_ls,\n",
    "                           replyto_ls, location_ls, datetime_ls, geo_loc_ls)),\n",
    "                  columns=['Username', 'UserID', 'Tweet',\n",
    "                           'Reply_to', 'Location', 'Date', 'Coordinates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we remove any data that do not contain tweets (empty text) - as they are not relevent for the scope of this project. Unfortunatey the free versio of TwitterAPI does not provide the option to filter out for tweets that do not contain text. This is an option that is available only for paid subscriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets that they did not have any text\n",
    "df = df[df['Tweet'].notnull()].reset_index()\n",
    "df.drop(columns=['index'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tweets are not written in the English language, so we are going to translate them by using the Google translate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateTweet(text: str) -> str:\n",
    "    '''\n",
    "    If Tweets are written in any other language than English, translate to\n",
    "    English and return the translated string.\n",
    "    '''\n",
    "    translator = Translator(service_urls=['translate.google.com'])\n",
    "    try:\n",
    "        textTranslated = translator.translate(text, dest='en').text\n",
    "    except json.JSONDecodeError:\n",
    "        textTranslated = text\n",
    "        pass\n",
    "    return textTranslated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language and translate if necessary\n",
    "df['Tweet'] = df['Tweet'].apply(lambda text: translateTweet(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweetsdata20200321.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweetsdata20200321.csv', sep='\\t', encoding = 'utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One good idea would be to attempt and plot the tweets in a geographical map. In order to do that we would need the coordinates (long/lat) of the tweets, something that is not available in most of the data extracted. That said, we can observe that most of the tweets have a 'Location' tag, that we can use and reverse engineer the long/lat data from the location string. For example, if a tweet has 'London' as a location string, we will transform this string to a corresponding longitude. latitude pair. \n",
    "\n",
    "For this purpose we will use the functionality that the geopy package has to offer, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geopy has a limit in the times we can call it per second so we have to find a workaround\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"https://developer.twitter.com/en/apps/17403833\") \n",
    "   \n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1,\n",
    "                      max_retries=3, error_wait_seconds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it in batches and identify the locations\n",
    "step = 100\n",
    "\n",
    "for batch in range(0, df.shape[0], step):\n",
    "    batchstep = batch+step\n",
    "    if batchstep > df.shape[0]:\n",
    "        batchstep = batch + (df.shape[0]%step)\n",
    "    print(f'\\nCalculating batch: {batch}-{batchstep}\\n')\n",
    "    df['Point'] = df['Location'][batch:batchstep].apply(lambda \n",
    "                                   loc: twf.getValidCoordinates(loc, geolocator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithCoords = df[df['Point'].notnull()]\n",
    "dfWithCoords['Latitude'] = dfWithCoords['Point'].apply(lambda x: x[0])\n",
    "dfWithCoords['Longitude'] = dfWithCoords['Point'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a function that will use the Longitude and Latitude values retrieved above, and plot them as a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objs as go\n",
    "\n",
    "def createTweetWorldMap(df: pd.core.frame.DataFrame):\n",
    "    '''\n",
    "    Given dataframe that contains columns corresponding to Longitude and Latitude,\n",
    "    create a world map plot and mark the Tweet locations on the map.\n",
    "    \n",
    "    '''\n",
    "    df['Text'] = df['Date'] + ': \\n' + df['Tweet']\n",
    "    \n",
    "    fig = go.Figure(data=go.Scattergeo(lon = df['Longitude'],\n",
    "                                       lat = df['Latitude'],\n",
    "                                       text = df['Text'],\n",
    "                                       mode = 'markers',\n",
    "                                       marker = dict(\n",
    "                                           symbol = 'circle',\n",
    "                                           line = dict(\n",
    "                                               width=1,\n",
    "                                               color='rgba(102, 102, 102)'\n",
    "                                           ),\n",
    "                                           colorscale = 'Viridis',\n",
    "                                           cmin = 0,\n",
    "            )))\n",
    "    \n",
    "    fig.update_layout(title = 'COVID-19 related Tweets across the world (January 2020 - March 2020) ',\n",
    "                      geo_scope='world',\n",
    "                      geo = dict(\n",
    "                          resolution = 110,\n",
    "                          scope = 'world',\n",
    "                          showland = True,\n",
    "                          landcolor = \"rgb(250, 250, 250)\",\n",
    "                          subunitcolor = \"rgb(217, 217, 217)\",\n",
    "                          countrycolor = \"rgb(217, 217, 217)\",\n",
    "                          countrywidth = 0.6,\n",
    "                          subunitwidth = 0.6,\n",
    "                      ))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pmap.createTweetWorldMap(dfWithCoords)\n",
    "plot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
