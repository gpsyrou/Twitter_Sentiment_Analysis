{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data and Sentiment Analysis on COVID-19 related Tweets via the TwitterAPI\n",
    "\n",
    "### Author: George Spyrou\n",
    "### Date: 26/09/2020\n",
    "\n",
    "<img src=\"../img/sentiment_image.jpeg\" alt=\"Sentiment Picture\" width=\"600\" height=\"400\">\n",
    "\n",
    "## Sections\n",
    "\n",
    "- <a href='#project_idea' style=\"text-decoration: none\">Introduction</a>\n",
    "- <a href='#data_retrieval' style=\"text-decoration: none\">Part 1: Twitter API and Data Retrieval</a>\n",
    "- <a href='#data_cleaning' style=\"text-decoration: none\">Part 2: Data Preprocessing and Cleaning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='project_idea'></a>\n",
    "## Project Idea\n",
    "\n",
    "Purpose of this project is to leverage the TwitterAPI functionality offered by Twitter, and conduct an analysis on tweets that are related with the SARS-CoV-2 virus - or as it's widely known as **COVID-19**. From now on and for an ease of use we will refer to the virus with the latter name, which in reality is the name of the decease. \n",
    "\n",
    "Initially, this project started as an exploratory task to learn how the TwitterAPI can be used to retrieve data (tweets) from the web, and how to use the tweepy and searchtweets python packages.\n",
    "\n",
    "After I managed to retrieve the data, I found myself really interested into digging deeper and get a better understanding of the data and they information they contain. COVID-19 is one of the most discussed topics in Twitter - or any other social media platform - roughly since the virus was first discovered in December of 2019. Because of the nature of this topic, we would expect that people have different opinions - some people feel more scared about the virus, some people do not even believe that the virus exists in the first place. Hence, I thought it would be interesting to see how the **overall** sentinemt of people's opinions about the virus is changing in different time periods during the year.\n",
    "\n",
    "I believe most people would agree that we are expecting that during the period January 2020 - March 2020, whilst the virus was not widely spread yet, people would not feel afraid and the sentiment would most likely be **neutral**. On the other hand, after April 2020, when most of the countries ended up having a lockdown and the virus became a reality for everyone, we would expect that the sentiment would be more **negative**.\n",
    "\n",
    "Now before we move further we have to make sure that we understand how we will _measure_ the sentiment and what it means for a Tweet to be **positive**/**neutral**/**negative**. The idea is actually pretty simple, as we will be looking at individual words that tweets consist of and try to understand the sentiment of the tweet as a whole. As a quick example, if a tweet contains words like 'death', 'decease' it's more likely to get a negative sentiment, compared to a tweet containing words like 'cure' and 'healed'.\n",
    "\n",
    "\n",
    "#### Version 1: The first version has been completed on 01/03/2020 and it includes analysis on:\n",
    "- Most common words present in tweets.\n",
    "- Most common bigrams (i.e. pairs of words that often appear next to each other).\n",
    "- Sentiment analysis by using the Liu Hu opinion lexicon algorithm.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_retrieval\"></a>\n",
    "## Part 1 - Twitter API and Data Retrieval\n",
    "\n",
    "At the first part of this project, we are going to deal with setting up the environment required for the analysis, as well as retrieving the data by using the TwitterAPI and leveraging the awesome _searchtweets_ package (https://pypi.org/project/searchtweets/) to connect with the API.\n",
    "\n",
    "Below I am going to present the script that I have used to get Tweets for different time intervals. At this point it's necessary to mention that I have used the free tier for the TwitterAPI - which of course it's coming with some limitations. One major was the amount of data that I can retrieve per month, as the free tier is providing the following:\n",
    "1. 25k tweets for the 30day tier (i.e. retrieve data from max 30 days ago from the moment you are making the API call)\n",
    "2. 5k tweets for the full archive (i.e. retrieve data from any day during the year)\n",
    "\n",
    "Later in the project we will discuss further about some other limitations of the free tier. Below I am presenting the script that I have used to make the API calls. Please note that this script can not run in a jupyter notebook instance, as it's set up to run from the command line. Either way I chose to present it as it can be useful to see the logic of how to set up the python script to make the API calls.\n",
    "\n",
    "This script has been formatted in a way that it's trying to get data during different times during the day. I have done this so that we \"randomize\" the tweets as much as possible, because we wanted to avoid cases where for example we would receive all the data from a Monday morning, where the news/tweets/etc would most probably talk about the same topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from datetime  import datetime, timedelta\n",
    "\n",
    "# Twitter API\n",
    "from searchtweets import load_credentials\n",
    "from searchtweets import gen_rule_payload\n",
    "from searchtweets import ResultStream\n",
    "\n",
    "\n",
    "# Secure location of the required keys to connect to the API\n",
    "# This config also contains the search query\n",
    "json_loc = r'D:\\GitHub\\Projects\\Twitter_Project\\Twitter_Topic_Modelling\\twitter_config.json'\n",
    "\n",
    "with open(json_loc) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# Project folder location and keys\n",
    "os.chdir(config[\"project_directory\"])\n",
    "\n",
    "# Custom functions created for the project\n",
    "import twitter_custom_functions as tcf\n",
    "\n",
    "keys_yaml_location = config[\"keys\"]\n",
    "\n",
    "# Load the credentials to get access to the API\n",
    "premium_search_args = load_credentials(filename=keys_yaml_location,\n",
    "                                       yaml_key=\"search_tweets_api_fullarchive\",\n",
    "                                       env_overwrite=False)\n",
    "print(premium_search_args)\n",
    "\n",
    "# Set tweet extraction period \n",
    "parser=argparse.ArgumentParser()\n",
    "parser.add_argument('fromDate', type=str)\n",
    "parser.add_argument('toDate', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.toDate <= args.fromDate:\n",
    "    print('The date range given is invalid. Please give correct from/to dates')\n",
    "    exit()\n",
    "\n",
    "daysList = [args.fromDate]\n",
    "\n",
    "print(f'Collecting Tweets from: {args.fromDate} to {args.toDate}')\n",
    "\n",
    "while args.fromDate != args.toDate:\n",
    "    date = datetime.strptime(args.fromDate, \"%Y-%m-%d\")\n",
    "    mod_date = date + timedelta(days=1)\n",
    "    incrementedDay = datetime.strftime(mod_date, \"%Y-%m-%d\")\n",
    "    daysList.append(incrementedDay)\n",
    "    \n",
    "    args.fromDate = incrementedDay\n",
    "    \n",
    "# Retrieve the data for each day from the API\n",
    "for day in daysList:\n",
    "    \n",
    "    dayNhourList = tcf.create_date_time_frame(day, hourSep=2)\n",
    "    \n",
    "    for hs in dayNhourList:\n",
    "        fromDate = hs[0]\n",
    "        toDate = hs[1]\n",
    "        # Create the searching rule for the stream\n",
    "        rule = gen_rule_payload(pt_rule=config['search_query'],\n",
    "                                from_date=fromDate,\n",
    "                                to_date=toDate ,\n",
    "                                results_per_call=100)\n",
    "\n",
    "        # Set up the stream\n",
    "        rs = ResultStream(rule_payload=rule, max_results=100,\n",
    "                          **premium_search_args)\n",
    "\n",
    "        # Create a .jsonl with the results of the Stream query\n",
    "        #file_date = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "        file_date = '_'.join(hs).replace(' ', '').replace(':','')\n",
    "        filename = os.path.join(config[\"outputFiles\"],\n",
    "                                f'twitter_30day_results_{file_date}.jsonl')\n",
    "    \n",
    "        # Write the data received from the API to a file\n",
    "        with open(filename, 'a', encoding='utf-8') as f:\n",
    "            cntr = 0\n",
    "            for tweet in rs.stream():\n",
    "                cntr += 1\n",
    "                if cntr % 100 == 0:\n",
    "                    n_str, cr_date = str(cntr), tweet['created_at']\n",
    "                    print(f'\\n {n_str}: {cr_date}')\n",
    "                json.dump(tweet, f)\n",
    "                f.write('\\n')\n",
    "        print(f'Created file {f}:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above had to be run *many* times, and each time the output were multiple jsonl files containing the data retrieved for the specified timeframe. Note that the timeframe was usually 2 or 3 days in one call. The reason for that is that there is a **limit** of tweets you can receive in one API call, as well as the number of calls you can make in an hour. \n",
    "\n",
    "As you can imagine, the process above has generated hundrends of json files, each one corresponding to a _specific_ time period of a _specific_ day. To make our life easier, we have created a script to merge the raw json files into a single .txt file that will be getting updated every time we receive new day and re-run the script. I am not going to present the code that completes this job here, but if you are interested in it you can find it <a href=\"https://github.com/gpsyrou/Twitter_Topic_Modelling/blob/master/utilities/merge_json_files.py\" alt=\"link_merge_json\" style=\"text-decoration: none\" >here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_cleaning\"></a>\n",
    "## Part 2 - Data Preprocessing and Cleaning\n",
    "\n",
    "After merging the json file into one single text file which contains all of the data retrieved, we can move to the data preprocessing part of the project. At this point, the data contained in the text file need to be transformed into something that will be handy to work with when we move later to the data analysis part.\n",
    "\n",
    "To get a better understanding of what's inside the text file, we can imagine that each row represents a different tweet along with a lot of other information, like the date of the tweet, the userid, the tweet itself, the location of the tweet (if any), and many more details.\n",
    "\n",
    "For the purposes of this project, we will filter the tweets in a way that we will retain only the following attributes:\n",
    "1. **screen_name**: Corresponds to the username of the person who submitted the tweet\n",
    "2. **id**: The id of the User\n",
    "3. **text**: The text representing the tweet. This is the main attribute of our project\n",
    "4. **location**\n",
    "5. **created_at**: Date that the tweet was created\n",
    "\n",
    "Please note that these are only a few of the details that someone could retrieve via the usage of the TwitterAPI. More information about the tweet objects can be found here: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "json_loc = r'D:\\GitHub\\Projects\\Twitter_Project\\Twitter_Topic_Modelling\\twitter_config.json'\n",
    "\n",
    "with open(json_loc) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "os.chdir(config[\"project_directory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.twitter_custom_functions as tcf\n",
    "from sentiment_class import TwitterSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_list_file_loc = r'all_tweets_list.txt'\n",
    "\n",
    "with open(all_tweets_list_file_loc, 'rb') as file:\n",
    "    allTweetsList = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe based on the relevant data from the full list of the received tweets\n",
    "user_ls, userid_ls, tweet_ls = [], [], []\n",
    "location_ls, datetime_ls, replyto_ls = [], [], []\n",
    "geo_loc_ls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will populate the lists that we will use to create the main dataframe of the project. Note that as we are looping through the lists, as the same time we will perform some data cleaning. Specifically we will remove the strings that represent links from the tweets, as they do not provide much information for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes URLs (strings that start with 'http\\\\ or htpps\\\\) from text\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        text: Input string the we want to remove the URL from.\n",
    "     \n",
    "    Returns:\n",
    "    -------\n",
    "    text: The input string clean from any URL.\n",
    "    \"\"\"\n",
    "\n",
    "    regex = r'http[0-9a-zA-Z\\\\/.:]+.'\n",
    "    urllinks = re.findall(regex, text)\n",
    "    if  urllinks != []:\n",
    "        for url in urllinks:\n",
    "            print(f'String removed: {url}')\n",
    "            if type(url) is tuple:\n",
    "                url = [x for x in url if x != '']\n",
    "            try:\n",
    "                text = text.replace(url,'')\n",
    "            except TypeError:\n",
    "                continue\n",
    "        return text\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet_dict in allTweetsList:\n",
    "    user_ls.append(tweet_dict['user']['screen_name'])\n",
    "    userid_ls.append(tweet_dict['user']['id'])\n",
    "    tweet_ls.append(remove_url(tweet_dict['text']))\n",
    "    location_ls.append(tweet_dict['user']['location'])\n",
    "    datetime_ls.append(tweet_dict['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create the first version of the main dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(list(zip(user_ls, userid_ls, tweet_ls, replyto_ls, location_ls, datetime_ls)),\n",
    "                  columns=['Username', 'UserID', 'Tweet', 'Location', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Reply_to</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt_next</td>\n",
       "      <td>3991108098</td>\n",
       "      <td>The government has issued a travel advisory as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chennai, India</td>\n",
       "      <td>Fri Jan 17 11:59:45 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gomez_grandes</td>\n",
       "      <td>1136279013763289088</td>\n",
       "      <td>RT @gomez_grandes: #Cuba China registra una se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Fri Jan 17 11:59:40 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fredygu1</td>\n",
       "      <td>46778936</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monterrey</td>\n",
       "      <td>Fri Jan 17 11:59:30 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AndriuAlanoca</td>\n",
       "      <td>614513903</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Fri Jan 17 11:59:28 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ginniasa</td>\n",
       "      <td>77748386</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Puerto Montt, Chile</td>\n",
       "      <td>Fri Jan 17 11:59:24 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nonatdexlol</td>\n",
       "      <td>843601394183651328</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>riverdale barn</td>\n",
       "      <td>Fri Jan 17 11:59:24 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Zeokon</td>\n",
       "      <td>1970523644</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Fri Jan 17 11:59:14 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>malena1083</td>\n",
       "      <td>796893205925076992</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guayaquil, Ecuador</td>\n",
       "      <td>Fri Jan 17 11:59:11 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>supriyabez</td>\n",
       "      <td>206092315</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Fri Jan 17 11:58:57 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ommzk</td>\n",
       "      <td>229737319</td>\n",
       "      <td>RT @MailOnline: Thailand announces a SECOND ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ToxiCity:</td>\n",
       "      <td>Fri Jan 17 11:58:52 +0000 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Username               UserID  \\\n",
       "0        dt_next           3991108098   \n",
       "1  gomez_grandes  1136279013763289088   \n",
       "2       fredygu1             46778936   \n",
       "3  AndriuAlanoca            614513903   \n",
       "4       ginniasa             77748386   \n",
       "5    nonatdexlol   843601394183651328   \n",
       "6         Zeokon           1970523644   \n",
       "7     malena1083   796893205925076992   \n",
       "8     supriyabez            206092315   \n",
       "9          ommzk            229737319   \n",
       "\n",
       "                                               Tweet  Reply_to  \\\n",
       "0  The government has issued a travel advisory as...       NaN   \n",
       "1  RT @gomez_grandes: #Cuba China registra una se...       NaN   \n",
       "2                                               None       NaN   \n",
       "3                                               None       NaN   \n",
       "4                                               None       NaN   \n",
       "5                                               None       NaN   \n",
       "6                                               None       NaN   \n",
       "7                                               None       NaN   \n",
       "8                                               None       NaN   \n",
       "9  RT @MailOnline: Thailand announces a SECOND ca...       NaN   \n",
       "\n",
       "              Location                            Date  \n",
       "0       Chennai, India  Fri Jan 17 11:59:45 +0000 2020  \n",
       "1                 None  Fri Jan 17 11:59:40 +0000 2020  \n",
       "2            Monterrey  Fri Jan 17 11:59:30 +0000 2020  \n",
       "3                Chile  Fri Jan 17 11:59:28 +0000 2020  \n",
       "4  Puerto Montt, Chile  Fri Jan 17 11:59:24 +0000 2020  \n",
       "5      riverdale barn   Fri Jan 17 11:59:24 +0000 2020  \n",
       "6                 None  Fri Jan 17 11:59:14 +0000 2020  \n",
       "7   Guayaquil, Ecuador  Fri Jan 17 11:59:11 +0000 2020  \n",
       "8                Delhi  Fri Jan 17 11:58:57 +0000 2020  \n",
       "9            ToxiCity:  Fri Jan 17 11:58:52 +0000 2020  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
