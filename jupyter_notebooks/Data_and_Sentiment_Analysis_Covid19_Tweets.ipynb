{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data and Sentiment Analysis on COVID-19 related Tweets via the TwitterAPI\n",
    "\n",
    "### Author: George Spyrou\n",
    "### Date: 26/09/2020\n",
    "\n",
    "<img src=\"../img/sentiment_image.jpeg\" alt=\"Sentiment Picture\" width=\"600\" height=\"400\">\n",
    "\n",
    "## Sections\n",
    "\n",
    "- <a href='#project_idea' style=\"text-decoration: none\">Introduction</a>\n",
    "- <a href='#data_retrieval' style=\"text-decoration: none\">Part 1: Twitter API and Data Retrieval</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='project_idea'></a>\n",
    "## Project Idea\n",
    "\n",
    "Purpose of this project is to leverage the TwitterAPI functionality offered by Twitter, and conduct an analysis on tweets that are related with the SARS-CoV-2 virus - or as it's widely known as **COVID-19**. From now on and for an ease of use we will refer to the virus with the latter name, which in reality is the name of the decease. \n",
    "\n",
    "Initially, this project started as an exploratory task to learn how the TwitterAPI can be used to retrieve data (tweets) from the web, and how to use the tweepy and searchtweets python packages.\n",
    "\n",
    "After I managed to retrieve the data, I found myself really interested into digging deeper and get a better understanding of the data and they information they contain. COVID-19 is one of the most discussed topics in Twitter - or any other social media platform - roughly since the virus was first discovered in December of 2019. Because of the nature of this topic, we would expect that people have different opinions - some people feel more scared about the virus, some people do not even believe that the virus exists in the first place. Hence, I thought it would be interesting to see how the **overall** sentinemt of people's opinions about the virus is changing in different time periods during the year.\n",
    "\n",
    "I believe most people would agree that we are expecting that during the period January 2020 - March 2020, whilst the virus was not widely spread yet, people would not feel afraid and the sentiment would most likely be **neutral**. On the other hand, after April 2020, when most of the countries ended up having a lockdown and the virus became a reality for everyone, we would expect that the sentiment would be more **negative**.\n",
    "\n",
    "Now before we move further we have to make sure that we understand how we will _measure_ the sentiment and what it means for a Tweet to be **positive**/**neutral**/**negative**. The idea is actually pretty simple, as we will be looking at individual words that tweets consist of and try to understand the sentiment of the tweet as a whole. As a quick example, if a tweet contains words like 'death', 'decease' it's more likely to get a negative sentiment, compared to a tweet containing words like 'cure' and 'healed'.\n",
    "\n",
    "\n",
    "#### Version 1: The first version has been completed on 01/03/2020 and it includes analysis on:\n",
    "- Most common words present in tweets.\n",
    "- Most common bigrams (i.e. pairs of words that often appear next to each other).\n",
    "- Sentiment analysis by using the Liu Hu opinion lexicon algorithm.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_retrieval\"></a>\n",
    "## Part 1 - Twitter API and Data Retrieval\n",
    "\n",
    "At the first part of this project, we are going to deal with setting up the environment required for the analysis, as well as retrieving the data by using the TwitterAPI and leveraging the awesome _searchtweets_ package (https://pypi.org/project/searchtweets/) to connect with the API.\n",
    "\n",
    "Below I am going to present the script that I have used to get Tweets for different time intervals. At this point it's necessary to mention that I have used the free tier for the TwitterAPI - which of course it's coming with some limitations. One major was the amount of data that I can retrieve per month, as the free tier is providing the following:\n",
    "1. 25k tweets for the 30day tier (i.e. retrieve data from max 30 days ago from the moment you are making the API call)\n",
    "2. 5k tweets for the full archive (i.e. retrieve data from any day during the year)\n",
    "\n",
    "Later in the project we will discuss further about some other limitations of the free tier. Below I am presenting the script that I have used to make the API calls. Please note that this script can not run in a jupyter notebook instance, as it's set up to run from the command line. Either way I chose to present it as it can be useful to see the logic of how to set up the python script to make the API calls.\n",
    "\n",
    "This script has been formatted in a way that it's trying to get data during different times during the day. I have done this so that we \"randomize\" the tweets as much as possible, because we wanted to avoid cases where for example we would receive all the data from a Monday morning, where the news/tweets/etc would most probably talk about the same topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from datetime  import datetime, timedelta\n",
    "\n",
    "# Twitter API\n",
    "from searchtweets import load_credentials\n",
    "from searchtweets import gen_rule_payload\n",
    "from searchtweets import ResultStream\n",
    "\n",
    "\n",
    "# Secure location of the required keys to connect to the API\n",
    "# This config also contains the search query\n",
    "json_loc = r'D:\\GitHub\\Projects\\Twitter_Project\\Twitter_Topic_Modelling\\twitter_config.json'\n",
    "\n",
    "with open(json_loc) as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# Project folder location and keys\n",
    "os.chdir(config[\"project_directory\"])\n",
    "\n",
    "# Custom functions created for the project\n",
    "import twitter_custom_functions as tcf\n",
    "\n",
    "keys_yaml_location = config[\"keys\"]\n",
    "\n",
    "# Load the credentials to get access to the API\n",
    "premium_search_args = load_credentials(filename=keys_yaml_location,\n",
    "                                       yaml_key=\"search_tweets_api_fullarchive\",\n",
    "                                       env_overwrite=False)\n",
    "print(premium_search_args)\n",
    "\n",
    "# Set tweet extraction period \n",
    "parser=argparse.ArgumentParser()\n",
    "parser.add_argument('fromDate', type=str)\n",
    "parser.add_argument('toDate', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.toDate <= args.fromDate:\n",
    "    print('The date range given is invalid. Please give correct from/to dates')\n",
    "    exit()\n",
    "\n",
    "daysList = [args.fromDate]\n",
    "\n",
    "print(f'Collecting Tweets from: {args.fromDate} to {args.toDate}')\n",
    "\n",
    "while args.fromDate != args.toDate:\n",
    "    date = datetime.strptime(args.fromDate, \"%Y-%m-%d\")\n",
    "    mod_date = date + timedelta(days=1)\n",
    "    incrementedDay = datetime.strftime(mod_date, \"%Y-%m-%d\")\n",
    "    daysList.append(incrementedDay)\n",
    "    \n",
    "    args.fromDate = incrementedDay\n",
    "    \n",
    "# Retrieve the data for each day from the API\n",
    "for day in daysList:\n",
    "    \n",
    "    dayNhourList = tcf.create_date_time_frame(day, hourSep=2)\n",
    "    \n",
    "    for hs in dayNhourList:\n",
    "        fromDate = hs[0]\n",
    "        toDate = hs[1]\n",
    "        # Create the searching rule for the stream\n",
    "        rule = gen_rule_payload(pt_rule=config['search_query'],\n",
    "                                from_date=fromDate,\n",
    "                                to_date=toDate ,\n",
    "                                results_per_call=100)\n",
    "\n",
    "        # Set up the stream\n",
    "        rs = ResultStream(rule_payload=rule, max_results=100,\n",
    "                          **premium_search_args)\n",
    "\n",
    "        # Create a .jsonl with the results of the Stream query\n",
    "        #file_date = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "        file_date = '_'.join(hs).replace(' ', '').replace(':','')\n",
    "        filename = os.path.join(config[\"outputFiles\"],\n",
    "                                f'twitter_30day_results_{file_date}.jsonl')\n",
    "    \n",
    "        # Write the data received from the API to a file\n",
    "        with open(filename, 'a', encoding='utf-8') as f:\n",
    "            cntr = 0\n",
    "            for tweet in rs.stream():\n",
    "                cntr += 1\n",
    "                if cntr % 100 == 0:\n",
    "                    n_str, cr_date = str(cntr), tweet['created_at']\n",
    "                    print(f'\\n {n_str}: {cr_date}')\n",
    "                json.dump(tweet, f)\n",
    "                f.write('\\n')\n",
    "        print(f'Created file {f}:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above had to be run *many* times, and each time the output were multiple jsonl files containing the data retrieved for the specified timeframe. Note that the timeframe was usually 2 or 3 days in one call. The reason for that is that there is a **limit** of tweets you can receive in one API call, as well as the number of calls you can make in an hour. \n",
    "\n",
    "As you can imagine, the process above has generated hundrends of json files, each one corresponding to a _specific_ time period of a _specific_ day. To make our life easier, we have created a script to merge the raw json files into a single .txt file that will be getting updated every time we receive new day and re-run the script. I am not going to present the code that completes this job here, but if you are interested in it you can find it <a href=\"https://github.com/gpsyrou/Twitter_Topic_Modelling/blob/master/utilities/merge_json_files.py\" alt=\"link_merge_json\" style=\"text-decoration: none\" >here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
